{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pyntcloud import PyntCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ply(file_name):\n",
    "    cloud = PyntCloud.from_file(file_name)\n",
    "    return cloud.points.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj(file_name):\n",
    "    vertices = []\n",
    "    with open(file_name) as f:\n",
    "        for line in f:\n",
    "            if line[:2] == 'v ':\n",
    "                index1 = line.find(' ') + 1\n",
    "                index2 = line.find(' ', index1 + 1)\n",
    "                index3 = line.find(' ', index2 + 1)\n",
    "\n",
    "                vertex = (float(line[index1:index2]), float(line[index2:index3]), float(line[index3:-1]))\n",
    "                vertex = [round(vertex[0], 2), round(vertex[1], 2), round(vertex[2], 2)]\n",
    "                vertices.append(np.array(vertex))\n",
    "                \n",
    "    return np.array(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_off(file_name):\n",
    "    vertices = []\n",
    "    with open(file_name) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            vals = line.split(' ')\n",
    "            if i > 2 and len(vals) == 3:\n",
    "                vertex = [float(vals[0]), float(vals[1]), float(vals[2])]\n",
    "                vertices.append(np.array(vertex))\n",
    "\n",
    "    return np.random.permutation(np.array(vertices))[:2048]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudDataset(Dataset):\n",
    "    \"\"\"Point cloud dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, number=-1, directory='./data/04379243'):\n",
    "        \n",
    "        file_names = glob.glob('%s/*.ply' % directory)\n",
    "\n",
    "        if number > 0 and len(file_names) > number:\n",
    "            file_names = file_names[:number]\n",
    "        \n",
    "        point_clouds = []\n",
    "        for file_name in file_names:\n",
    "\n",
    "            points = load_ply(file_name)\n",
    "            point_clouds.append(points)\n",
    "\n",
    "        self.point_clouds = np.array(point_clouds, dtype='float64')\n",
    "        self.point_clouds = np.transpose(self.point_clouds, (0, 2, 1))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.point_clouds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.point_clouds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_dataset = PointCloudDataset(200)\n",
    "dataloader = DataLoader(point_cloud_dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
       "  (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "  (conv3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
       "  (conv4): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "  (conv5): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "  (maxPool1d): MaxPool1d(kernel_size=2048, stride=2048, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 1)\n",
    "        self.conv4 = nn.Conv1d(256, 256, 1)\n",
    "        self.conv5 = nn.Conv1d(256, 128, 1)\n",
    "        self.maxPool1d = nn.MaxPool1d(2048)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1  = nn.BatchNorm1d(64) \n",
    "        self.bn2  = nn.BatchNorm1d(128) \n",
    "        self.bn3  = nn.BatchNorm1d(256)\n",
    "        self.bn4  = nn.BatchNorm1d(256) \n",
    "        self.bn5  = nn.BatchNorm1d(128) \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.maxPool1d(x)\n",
    "        return x\n",
    "\n",
    "Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (fc1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=6144, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 2048*3)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1  = nn.BatchNorm1d(256) \n",
    "        self.bn2  = nn.BatchNorm1d(256) \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        x = x.view(-1, 3, 2048)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [30/2000], loss:4392.4868\n",
      "epoch [60/2000], loss:3955.4326\n",
      "epoch [90/2000], loss:3893.6924\n",
      "epoch [120/2000], loss:3883.5156\n",
      "epoch [150/2000], loss:3938.4954\n",
      "epoch [180/2000], loss:3996.1606\n",
      "epoch [210/2000], loss:3765.2651\n",
      "epoch [240/2000], loss:4019.3213\n",
      "epoch [270/2000], loss:3817.7510\n",
      "epoch [300/2000], loss:4155.7900\n",
      "epoch [330/2000], loss:3767.2783\n",
      "epoch [360/2000], loss:3387.7898\n",
      "epoch [390/2000], loss:4136.5718\n",
      "epoch [420/2000], loss:3715.8105\n",
      "epoch [450/2000], loss:3861.6033\n",
      "epoch [480/2000], loss:3774.4512\n",
      "epoch [510/2000], loss:3724.6223\n",
      "epoch [540/2000], loss:3850.7087\n",
      "epoch [570/2000], loss:3636.8374\n",
      "epoch [600/2000], loss:3554.3472\n",
      "epoch [630/2000], loss:3728.0703\n",
      "epoch [660/2000], loss:4132.7754\n",
      "epoch [690/2000], loss:3646.8154\n",
      "epoch [720/2000], loss:3652.8970\n",
      "epoch [750/2000], loss:3502.6401\n",
      "epoch [780/2000], loss:3624.4683\n",
      "epoch [810/2000], loss:3508.3906\n",
      "epoch [840/2000], loss:3527.9639\n",
      "epoch [870/2000], loss:3849.3491\n",
      "epoch [900/2000], loss:3887.4478\n",
      "epoch [930/2000], loss:3918.4712\n",
      "epoch [960/2000], loss:3755.7336\n",
      "epoch [990/2000], loss:3830.2722\n",
      "epoch [1020/2000], loss:3923.7148\n",
      "epoch [1050/2000], loss:3868.3774\n",
      "epoch [1080/2000], loss:3494.7661\n",
      "epoch [1110/2000], loss:3763.9541\n",
      "epoch [1140/2000], loss:3646.8225\n",
      "epoch [1170/2000], loss:4030.3159\n",
      "epoch [1200/2000], loss:4022.5784\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-4819d22a93db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda2\\envs\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda2\\envs\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lib.chamfer import ChamferLoss\n",
    "\n",
    "model = AutoEncoder()\n",
    "\n",
    "criterion = ChamferLoss()\n",
    "\n",
    "num_epochs = 2000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "if torch.cuda.is_available:\n",
    "    model = model.cuda()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for data in dataloader:\n",
    "\n",
    "        data = data.float()\n",
    "        \n",
    "        if torch.cuda.is_available:\n",
    "            data = data.cuda()\n",
    "            \n",
    "        train_output = model(data)\n",
    "        loss = criterion(train_output, data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch + 1) % 30 == 0:\n",
    "        print('epoch [{}/{}], loss:{:.4f}'.format(epoch + 1, num_epochs, loss.data.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), './models/encoder.pt')\n",
    "torch.save(model.decoder.state_dict(), './models/decoder.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get input values for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input  = next(iter(dataloader)).float()\n",
    "\n",
    "if torch.cuda.is_available :\n",
    "    train_output = model(train_input.cuda())\n",
    "    train_output = train_output.cpu()\n",
    "else:\n",
    "    train_output = model(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_encoder = Encoder()\n",
    "v_decoder = Decoder()\n",
    "\n",
    "v_encoder.load_state_dict(torch.load('./models/encoder.pt'))\n",
    "v_decoder.load_state_dict(torch.load('./models/decoder.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        x[:,1,0] *= 10.0\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(v_encoder, v_decoder)\n",
    "generator_output = generator(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 8))\n",
    "\n",
    "for i in range(8):\n",
    "    \n",
    "    row, columns, num = 2, 4, i + 1\n",
    "    ax = fig.add_subplot(row, columns, num, projection='3d')\n",
    "\n",
    "    data = train_input[i].detach().numpy()\n",
    "    ax.scatter(data[0], data[1], data[2], zdir='z', s=4, c='b', label='ground truth')\n",
    "\n",
    "    data = train_output[i].detach().numpy()\n",
    "    ax.scatter(data[0], data[1], data[2], zdir='z', s=4, c='r', label='decoded')\n",
    "\n",
    "    data = generator_output[i].detach().numpy()\n",
    "    ax.scatter(data[0], data[1], data[2], zdir='z', s=4, c='g', label='decoded & modified')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(points, file_name, directory='./output/'):\n",
    "    \n",
    "    if points.shape[0] < points.shape[1] and points.shape[0] == 3:\n",
    "        points = points.T\n",
    "    \n",
    "    with open(directory + file_name, 'w') as f:\n",
    "        \n",
    "        f.write(str(points.shape[0]) + '\\n')\n",
    "\n",
    "        for pt in points:\n",
    "            f.write('%.8f %.8f %.8f\\n' % tuple(pt))\n",
    "    \n",
    "write(next(iter(dataloader))[2].detach().numpy(), 'point_cloud.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "* Manimulate latent vector or try something else to generate more nice tables\n",
    "* Reduce the number of point clouds or connect each points reasonably"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(10, 2048, 3)\n",
    "print(input.shape)\n",
    "m = nn.Conv1d(2048, 256, 1)\n",
    "output = m(input)\n",
    "print(output.shape)\n",
    "m = nn.Conv1d(256, 128, 1)\n",
    "output = m(output)\n",
    "m = nn.Conv1d(128, 64, 1)\n",
    "output = m(output)\n",
    "print(output.shape)\n",
    "m = nn.Conv1d(64, 32, 1)\n",
    "output = m(output)\n",
    "print(output.shape)\n",
    "m = nn.MaxPool1d(3)\n",
    "output = m(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(10, 3, 2048)\n",
    "print(input.shape)\n",
    "m = nn.Conv1d(3, 64, 1)\n",
    "output = m(input)\n",
    "print(output.shape)\n",
    "m = nn.Conv1d(64, 128, 1)\n",
    "output = m(output)\n",
    "m = nn.Conv1d(128, 256, 1)\n",
    "output = m(output)\n",
    "print(output.shape)\n",
    "m = nn.Conv1d(256, 512, 1)\n",
    "output = m(output)\n",
    "print(output.shape)\n",
    "m = nn.MaxPool1d(2048) # output = torch.max(output, 2, keepdim=True)[0]\n",
    "output = m(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Variable(torch.rand(32,3,2500))\n",
    "print(input.shape)\n",
    "m = torch.nn.Conv1d(3, 64, 1)\n",
    "output = m(input)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
